{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding reading all das files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "base_directory='/home/osman/aramko/'\n",
    "das_directory=base_directory+'inputs/das'\n",
    "rep=base_directory+'/outputs/das/h5_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.7 ms\n"
     ]
    }
   ],
   "source": [
    "def h5_reader(f):\n",
    "    return h5py.File(f, 'r')\n",
    "\n",
    "def time_extracter(h5file):\n",
    "    return int(h5file['t'][0])\n",
    "\n",
    "def das_extracter(f):\n",
    "    return pd.DataFrame(np.array(h5_reader(f).get('das')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1b9b054fb5468d90bd43b290b037b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Pandas Apply'), FloatProgress(value=0.0, max=918.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "918"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 114 ms\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import swifter\n",
    "import numpy as np\n",
    "import sys\n",
    "from scipy import signal\n",
    "files=glob.glob(das_directory+\"/*.h5\")\n",
    "filesdf=pd.DataFrame(files,columns=['file'])\n",
    "chanels=list(h5_reader(filesdf['file'][0])['channel'])\n",
    "filesdf['name']=filesdf['file'].swifter.apply(lambda x:x.split('/')[-1].replace('.h5',''))\n",
    "chanels=list(h5_reader(filesdf['file'][0])['channel'])\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding missed data in das"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.63 ms\n"
     ]
    }
   ],
   "source": [
    "def filename_extracter(last_file_name):\n",
    "        if last_file_name[-2:]=='17':\n",
    "            return str(int(last_file_name)+30)\n",
    "        else :\n",
    "            return str(int(last_file_name)+70)\n",
    "#we noticed the wasy the data file name is ....         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45a8cdbf4bc4e35a8a5c85fa6d8f628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Pandas Apply'), FloatProgress(value=0.0, max=918.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>1458306107</td>\n",
       "      <td>160318130147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1458306137</td>\n",
       "      <td>160318130217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>1458306167</td>\n",
       "      <td>160318130247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1458306197</td>\n",
       "      <td>160318130317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>1458306227</td>\n",
       "      <td>160318130347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>1458324047</td>\n",
       "      <td>160318176047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>1458324077</td>\n",
       "      <td>160318176117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>1458324107</td>\n",
       "      <td>160318176147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>1458324137</td>\n",
       "      <td>160318176217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>1458324167</td>\n",
       "      <td>160318176247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           start          name\n",
       "483   1458306107  160318130147\n",
       "484   1458306137  160318130217\n",
       "485   1458306167  160318130247\n",
       "486   1458306197  160318130317\n",
       "487   1458306227  160318130347\n",
       "...          ...           ...\n",
       "1081  1458324047  160318176047\n",
       "1082  1458324077  160318176117\n",
       "1083  1458324107  160318176147\n",
       "1084  1458324137  160318176217\n",
       "1085  1458324167  160318176247\n",
       "\n",
       "[283 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "filesdf['start']=filesdf['file'].swifter.apply(lambda x:time_extracter(h5_reader(x)))\n",
    "filesdf=filesdf.sort_values('start').reset_index(drop=True)\n",
    "all_files=pd.DataFrame(list(range(filesdf.start.min(),filesdf.start.max(),30)),columns=['start']).copy()\n",
    "all_files=all_files.merge(filesdf,how='left',on='start')\n",
    "all_files['last_file_name']=all_files['name'].shift(periods=1)\n",
    "all_files=all_files.fillna('-1')\n",
    "all_files['output']=0\n",
    "for i in all_files[all_files.name=='-1'].index:\n",
    "    all_files.loc[i, 'name'] = filename_extracter(all_files.loc[i, 'last_file_name'])\n",
    "    all_files.loc[i+1, 'last_file_name']=all_files.loc[i, 'name']\n",
    "    all_files.loc[i, 'output']=1\n",
    "all_files[all_files.output==1][['start','name']] #thats the time missing parts with the names generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting das files by chanels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 576 µs\n"
     ]
    }
   ],
   "source": [
    "numsimp=int(30000/100)\n",
    "def signal_preprocessing(sig):\n",
    "    lowpass_filter = signal.butter(1,Wn=0.001,btype='low',output='sos')\n",
    "    a, b = signal.welch(signal.resample((signal.sosfilt(lowpass_filter,sig)),int(numsimp)), fs=0.1, scaling='spectrum')\n",
    "    return b\n",
    "\n",
    "def preprocessing_generale(file):    \n",
    "    data=[]\n",
    "    tmp=das_extracter(file)\n",
    "    for i in tmp.columns:\n",
    "        data.append(signal_preprocessing(tmp[i]))\n",
    "    result=pd.DataFrame([data])\n",
    "    result['file']=file\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool(num_cores) as pool:\n",
    "    data_by_chanel = pd.concat(pool.map(preprocessing_generale, filesdf['file']), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# starting processing data by channel for TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from cuml.decomposition import PCA as cuPCA\n",
    "from cuml import DBSCAN as cumlDBSCAN\n",
    "scaler= MinMaxScaler()\n",
    "pca_cuml = cuPCA(n_components=3,random_state=42)\n",
    "def signal_transform(filesdf,chanel):\n",
    "    result_cuml = pca_cuml.fit_transform(scaler.fit_transform(pd.DataFrame(filesdf[chanel].values.tolist())))\n",
    "    data_by_chanel[chanel]=pd.DataFrame(pd.DataFrame(result_cuml).apply(lambda r: tuple(r), axis=1).apply(np.array))\n",
    "    return result_cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "# appliying PCA on the signale \n",
    "dataset=np.concatenate([signal_transform(data_by_chanel,i) for i in range(len(chanels))],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 45.1 ms\n"
     ]
    }
   ],
   "source": [
    "#small trick to access rapidly to the missed data\n",
    "all_files=all_files.merge(data_by_chanel,how='left',on='file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.9 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def shifter(dataset,look_back):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "            a = dataset[i:(i + look_back),:]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + look_back, :])\n",
    "    x=np.asarray(dataX).reshape(len(dataX),look_back,len(dataY[1])) \n",
    "    y=np.asarray(dataY).reshape(len(dataY),len(dataY[1]))\n",
    "    return train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "def preprocessing_training(chanels):\n",
    "    window=20\n",
    "    x_train, x_test, y_train, y_test = shifter(dataset,window)\n",
    "    model= trainer(window,x_train, x_test, y_train, y_test)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.2 ms\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Dropout\n",
    "from keras.optimizers import  Adam,RMSprop\n",
    "from keras.callbacks import TensorBoard\n",
    "config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1} ) \n",
    "sess = tf.compat.v1.Session(config=config) \n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "rm=RMSprop(\n",
    "    learning_rate=0.005,\n",
    ")\n",
    "def trainer(window,x_train, x_test, y_train, y_test):\n",
    "    shape=3\n",
    "    size=300\n",
    "    drops=0.2\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(size,input_shape=(window,shape),return_sequences=True))\n",
    "    model.add(Dropout(drops))\n",
    "    model.add(LSTM(size,return_sequences=False))\n",
    "    model.add(Dropout(drops))\n",
    "    model.add(Dense(shape))\n",
    "    model.compile(loss='mae', optimizer=rm,metrics=['cosine_similarity'])\n",
    "    model.fit(x_train, y_train, epochs=50,verbose=2,validation_split=0.2,batch_size=600)\n",
    "    print('model score : ',model.evaluate(x_test, y_test,verbose=2)[1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "376/376 - 19s - loss: 0.1049 - cosine_similarity: 0.5185 - val_loss: 0.0888 - val_cosine_similarity: 0.5942\n",
      "Epoch 2/50\n",
      "376/376 - 18s - loss: 0.0890 - cosine_similarity: 0.6019 - val_loss: 0.0836 - val_cosine_similarity: 0.6234\n",
      "Epoch 3/50\n",
      "376/376 - 18s - loss: 0.0851 - cosine_similarity: 0.6295 - val_loss: 0.0801 - val_cosine_similarity: 0.6447\n",
      "Epoch 4/50\n",
      "376/376 - 18s - loss: 0.0823 - cosine_similarity: 0.6493 - val_loss: 0.0785 - val_cosine_similarity: 0.6663\n",
      "Epoch 5/50\n",
      "376/376 - 18s - loss: 0.0797 - cosine_similarity: 0.6682 - val_loss: 0.0763 - val_cosine_similarity: 0.6737\n",
      "Epoch 6/50\n",
      "376/376 - 18s - loss: 0.0772 - cosine_similarity: 0.6856 - val_loss: 0.0744 - val_cosine_similarity: 0.6904\n",
      "Epoch 7/50\n",
      "376/376 - 18s - loss: 0.0750 - cosine_similarity: 0.7014 - val_loss: 0.0714 - val_cosine_similarity: 0.7060\n",
      "Epoch 8/50\n",
      "376/376 - 18s - loss: 0.0727 - cosine_similarity: 0.7172 - val_loss: 0.0705 - val_cosine_similarity: 0.7176\n",
      "Epoch 9/50\n",
      "376/376 - 18s - loss: 0.0708 - cosine_similarity: 0.7312 - val_loss: 0.0686 - val_cosine_similarity: 0.7320\n",
      "Epoch 10/50\n",
      "376/376 - 18s - loss: 0.0690 - cosine_similarity: 0.7415 - val_loss: 0.0677 - val_cosine_similarity: 0.7408\n",
      "Epoch 11/50\n",
      "376/376 - 18s - loss: 0.0673 - cosine_similarity: 0.7509 - val_loss: 0.0668 - val_cosine_similarity: 0.7430\n",
      "Epoch 12/50\n",
      "376/376 - 18s - loss: 0.0658 - cosine_similarity: 0.7590 - val_loss: 0.0647 - val_cosine_similarity: 0.7571\n",
      "Epoch 13/50\n",
      "376/376 - 18s - loss: 0.0643 - cosine_similarity: 0.7663 - val_loss: 0.0636 - val_cosine_similarity: 0.7612\n",
      "Epoch 14/50\n",
      "376/376 - 18s - loss: 0.0631 - cosine_similarity: 0.7732 - val_loss: 0.0631 - val_cosine_similarity: 0.7634\n",
      "Epoch 15/50\n",
      "376/376 - 18s - loss: 0.0620 - cosine_similarity: 0.7794 - val_loss: 0.0629 - val_cosine_similarity: 0.7635\n",
      "Epoch 16/50\n",
      "376/376 - 18s - loss: 0.0610 - cosine_similarity: 0.7852 - val_loss: 0.0624 - val_cosine_similarity: 0.7711\n",
      "Epoch 17/50\n",
      "376/376 - 18s - loss: 0.0601 - cosine_similarity: 0.7909 - val_loss: 0.0611 - val_cosine_similarity: 0.7756\n",
      "Epoch 18/50\n",
      "376/376 - 18s - loss: 0.0593 - cosine_similarity: 0.7952 - val_loss: 0.0605 - val_cosine_similarity: 0.7809\n",
      "Epoch 19/50\n",
      "376/376 - 18s - loss: 0.0584 - cosine_similarity: 0.8003 - val_loss: 0.0603 - val_cosine_similarity: 0.7809\n",
      "Epoch 20/50\n",
      "376/376 - 18s - loss: 0.0577 - cosine_similarity: 0.8037 - val_loss: 0.0595 - val_cosine_similarity: 0.7857\n",
      "Epoch 21/50\n",
      "376/376 - 18s - loss: 0.0570 - cosine_similarity: 0.8082 - val_loss: 0.0604 - val_cosine_similarity: 0.7808\n",
      "Epoch 22/50\n",
      "376/376 - 18s - loss: 0.0563 - cosine_similarity: 0.8111 - val_loss: 0.0596 - val_cosine_similarity: 0.7896\n",
      "Epoch 23/50\n",
      "376/376 - 18s - loss: 0.0557 - cosine_similarity: 0.8147 - val_loss: 0.0590 - val_cosine_similarity: 0.7903\n",
      "Epoch 24/50\n",
      "376/376 - 18s - loss: 0.0552 - cosine_similarity: 0.8169 - val_loss: 0.0579 - val_cosine_similarity: 0.7942\n",
      "Epoch 25/50\n",
      "376/376 - 18s - loss: 0.0546 - cosine_similarity: 0.8208 - val_loss: 0.0574 - val_cosine_similarity: 0.7948\n",
      "Epoch 26/50\n",
      "376/376 - 18s - loss: 0.0540 - cosine_similarity: 0.8237 - val_loss: 0.0573 - val_cosine_similarity: 0.7951\n",
      "Epoch 27/50\n",
      "376/376 - 18s - loss: 0.0536 - cosine_similarity: 0.8262 - val_loss: 0.0573 - val_cosine_similarity: 0.7971\n",
      "Epoch 28/50\n",
      "376/376 - 18s - loss: 0.0532 - cosine_similarity: 0.8290 - val_loss: 0.0574 - val_cosine_similarity: 0.7973\n",
      "Epoch 29/50\n",
      "376/376 - 18s - loss: 0.0527 - cosine_similarity: 0.8318 - val_loss: 0.0567 - val_cosine_similarity: 0.7990\n",
      "Epoch 30/50\n",
      "376/376 - 18s - loss: 0.0523 - cosine_similarity: 0.8338 - val_loss: 0.0566 - val_cosine_similarity: 0.8016\n",
      "Epoch 31/50\n",
      "376/376 - 18s - loss: 0.0519 - cosine_similarity: 0.8360 - val_loss: 0.0564 - val_cosine_similarity: 0.8032\n",
      "Epoch 32/50\n",
      "376/376 - 18s - loss: 0.0515 - cosine_similarity: 0.8384 - val_loss: 0.0563 - val_cosine_similarity: 0.8027\n",
      "Epoch 33/50\n",
      "376/376 - 18s - loss: 0.0512 - cosine_similarity: 0.8404 - val_loss: 0.0559 - val_cosine_similarity: 0.8053\n",
      "Epoch 34/50\n",
      "376/376 - 18s - loss: 0.0507 - cosine_similarity: 0.8426 - val_loss: 0.0559 - val_cosine_similarity: 0.8057\n",
      "Epoch 35/50\n",
      "376/376 - 18s - loss: 0.0503 - cosine_similarity: 0.8446 - val_loss: 0.0553 - val_cosine_similarity: 0.8109\n",
      "Epoch 36/50\n",
      "376/376 - 18s - loss: 0.0501 - cosine_similarity: 0.8461 - val_loss: 0.0555 - val_cosine_similarity: 0.8075\n",
      "Epoch 37/50\n",
      "376/376 - 18s - loss: 0.0498 - cosine_similarity: 0.8478 - val_loss: 0.0553 - val_cosine_similarity: 0.8087\n",
      "Epoch 38/50\n",
      "376/376 - 18s - loss: 0.0494 - cosine_similarity: 0.8494 - val_loss: 0.0547 - val_cosine_similarity: 0.8121\n",
      "Epoch 39/50\n",
      "376/376 - 18s - loss: 0.0493 - cosine_similarity: 0.8509 - val_loss: 0.0551 - val_cosine_similarity: 0.8090\n",
      "Epoch 40/50\n",
      "376/376 - 18s - loss: 0.0489 - cosine_similarity: 0.8528 - val_loss: 0.0556 - val_cosine_similarity: 0.8098\n",
      "Epoch 41/50\n",
      "376/376 - 18s - loss: 0.0486 - cosine_similarity: 0.8540 - val_loss: 0.0545 - val_cosine_similarity: 0.8125\n",
      "Epoch 42/50\n",
      "376/376 - 18s - loss: 0.0483 - cosine_similarity: 0.8560 - val_loss: 0.0542 - val_cosine_similarity: 0.8140\n",
      "Epoch 43/50\n",
      "376/376 - 18s - loss: 0.0480 - cosine_similarity: 0.8577 - val_loss: 0.0547 - val_cosine_similarity: 0.8135\n",
      "Epoch 44/50\n",
      "376/376 - 18s - loss: 0.0478 - cosine_similarity: 0.8589 - val_loss: 0.0545 - val_cosine_similarity: 0.8165\n",
      "Epoch 45/50\n",
      "376/376 - 18s - loss: 0.0475 - cosine_similarity: 0.8591 - val_loss: 0.0538 - val_cosine_similarity: 0.8170\n",
      "Epoch 46/50\n",
      "376/376 - 18s - loss: 0.0473 - cosine_similarity: 0.8611 - val_loss: 0.0543 - val_cosine_similarity: 0.8154\n",
      "Epoch 47/50\n",
      "376/376 - 18s - loss: 0.0470 - cosine_similarity: 0.8616 - val_loss: 0.0546 - val_cosine_similarity: 0.8153\n",
      "Epoch 48/50\n",
      "376/376 - 18s - loss: 0.0468 - cosine_similarity: 0.8639 - val_loss: 0.0541 - val_cosine_similarity: 0.8157\n",
      "Epoch 49/50\n",
      "376/376 - 18s - loss: 0.0465 - cosine_similarity: 0.8649 - val_loss: 0.0540 - val_cosine_similarity: 0.8182\n",
      "Epoch 50/50\n",
      "376/376 - 18s - loss: 0.0463 - cosine_similarity: 0.8656 - val_loss: 0.0540 - val_cosine_similarity: 0.8157\n",
      "2204/2204 - 7s - loss: 0.0542 - cosine_similarity: 0.8146\n",
      "model score :  0.814567506313324\n",
      "time: 15min 6s\n"
     ]
    }
   ],
   "source": [
    "model=preprocessing_training(range(len(chanels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# starting prediction of lost data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.09 ms\n"
     ]
    }
   ],
   "source": [
    "def extract_rows(end,window=20,features=3):\n",
    "    all_results=[]\n",
    "    segment=all_files[end-window:end][range(len(chanels))].copy().reset_index(drop=True)\n",
    "    for ch in range(len(chanels)):\n",
    "        try:\n",
    "            all_results.append(np.concatenate([segment[ch][i] for i in range(len(segment[ch]))],axis=0).reshape(1,window,features))\n",
    "        except:\n",
    "            print (segment)   \n",
    "            \n",
    "    return np.concatenate([i for i in all_results],axis=0)\n",
    "\n",
    "def restorer(chanel,i,resutls):\n",
    "    all_files[chanel].loc[i]=resutls[chanel].copy()\n",
    "\n",
    "def predicter(end,window=20,features=3):\n",
    "    resutls=model.predict(extract_rows(end))\n",
    "    for ch in range(len(chanels)):  \n",
    "        restorer(ch,end,resutls)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osman/anaconda3/envs/rapids-0.16/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 44.1 s\n"
     ]
    }
   ],
   "source": [
    "for i in all_files[all_files['output']==1].index:\n",
    "    predicter(i) #predict all the missed data one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearestNeighbors algorithm used to make the new generted data like the same in dataset to avoid outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 480 µs\n"
     ]
    }
   ],
   "source": [
    "from cuml.neighbors import NearestNeighbors\n",
    "def get_nearest(chanel) :\n",
    "    NN=NearestNeighbors(n_neighbors=1)\n",
    "    NN.fit(pd.DataFrame(extract[chanel].values.tolist()))\n",
    "    output[chanel]=NN.kneighbors(pd.DataFrame(output[chanel].values.tolist()),return_distance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating template for missing data in das with the names they should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd593f95ab04b699fae548211e4d302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Pandas Apply'), FloatProgress(value=0.0, max=1200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "all_files['file']=all_files.swifter.apply(lambda x:generating_missed_files(filesdf['file'][0], x['name'],x['start']) if x['output']==1 else x['file'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 44.8 s\n"
     ]
    }
   ],
   "source": [
    "output=all_files[all_files['output']==1].copy().reset_index(drop=True)\n",
    "for ch in  range(len(chanels)):get_nearest(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.85 ms\n"
     ]
    }
   ],
   "source": [
    "def generating_missed_files(exemple, dest,t,rep=rep):\n",
    "    dest=rep+dest+'.h'\n",
    "    with h5py.File(exemple, 'r') as f1:\n",
    "        with h5py.File(dest, 'w') as f2:\n",
    "            for ds in f1.keys():\n",
    "                f1.copy(ds, f2)\n",
    "            f2['t'][:]=np.array(range(t,t+30000))\n",
    "    f2.close()\n",
    "    f1.close()\n",
    "    return dest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# writing final results data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.94 ms\n"
     ]
    }
   ],
   "source": [
    "def das_ins(index,chanel):\n",
    "    with h5py.File(output.loc[index]['file'], 'r+') as f1:\n",
    "        with h5py.File(filesdf['file'][output.loc[index][chanel]], 'r') as f2:\n",
    "            f1['das'][:,chanel]=f2['das'][:,chanel]\n",
    "    f1.close()\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from functools import partial\n",
    "for i in range(len(chanels)):\n",
    "    with multiprocessing.Pool(4) as pool:\n",
    "        pool.map(partial(das_ins, chanel=i), range(len(output)))\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
